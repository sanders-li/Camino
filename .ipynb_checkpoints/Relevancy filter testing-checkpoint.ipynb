{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problematic places:\n",
    "    \n",
    "Kaminarimon Gate: Wikipedia article titled \"Kaminarimon\", suggested \"Senso-ji\" place titled \"Karinarimon Gate Senso-Ji\"\n",
    "Fushimi-yagura keep: Wikipedia article \"Edo castle\". Perhaps extract summary for relevancy\n",
    "Nakamise shopping strret: Wikipedia artcile \"Senso-ji\". \n",
    "\n",
    "Results:\n",
    "Kaminarimon - Relevancy filter position 0, normal position 0\n",
    "Fushimi-yagura keep - Relevacy filter empty, normal position 0\n",
    "Nakamise shopping street - Relevancy filter empty, normal position 0. Is repeat of source\n",
    "Kasairinkai park - Relevancy filter not empty, but incorrect. Suggestion exists and is useful. \n",
    "\n",
    "Solutions:\n",
    "Check if suggestion exists. If yes, try it. \n",
    "If looping occurs, perform a wikipedia search instead of directly going to the page\n",
    "Relevancy filter results. If filter has results, pick first result. If empty, resort to first in returned results list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "\n",
    "def stringsubset(a, b):\n",
    "    '''\n",
    "    Returns number of substring matches of strings in A to strings in B\n",
    "    '''\n",
    "    #filter diacritics    \n",
    "    if isinstance(a, str):\n",
    "        a = unidecode(a)\n",
    "        a = [word.lower() for word in a.split()]\n",
    "    else:\n",
    "        a = [unidecode(word).lower() for word in a]\n",
    "    if isinstance(b, str):\n",
    "        b = unidecode(b)\n",
    "        b = [word.lower() for word in b.split()]\n",
    "    else:\n",
    "        b = [unidecode(word).lower() for word in b]\n",
    "    score = 0\n",
    "    for word_a in a:\n",
    "        for word_b in b:\n",
    "            if (word_a in word_b):\n",
    "                score += 1\n",
    "    return score\n",
    "\n",
    "def lcs(i, j, X, Y, count):\n",
    "    if (i == 0 or j == 0) :  \n",
    "        return count   \n",
    "    if (X[i - 1] == Y[j - 1]) : \n",
    "        count = lcs(i - 1, j - 1, X, Y, count + 1)  \n",
    "      \n",
    "    count = max(count, max(lcs(i, j - 1, X, Y, 0),  \n",
    "                           lcs(i - 1, j, X, Y, 0)))  \n",
    "  \n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "\n",
    "queries = ['Fushimi Inari Taisha', 'kyoto']\n",
    "results = ['Fushimi Inari-taisha', 'Inari shrine', 'Inari Ōkami', 'Fushimi-ku, Kyoto', 'Torii', 'Shinto', 'Star Fox (1993 video game)', 'Shinto shrine', 'Tenko (fox)', 'Inari, Konkon, Koi Iroha']\n",
    "\n",
    "queries = ['Kaminarimon Gate Senso-ji', 'tokyo']\n",
    "results = ['Sensō-ji', 'Kaminarimon', 'Asakusa', '10th century in architecture', 'Hōzōmon', 'Taitō', '941', 'Hirakushi Denchū', 'One Hundred Famous Views of Edo', '940s']\n",
    "    \n",
    "def relevancy_filter(queries, results):\n",
    "    priority_dict = {}\n",
    "    for result in results:\n",
    "        word_list = list(chain(*[words.split() for words in queries]))\n",
    "        matches = stringsubset(word_list, result.split())\n",
    "        if matches:\n",
    "            if priority_dict.get(matches):\n",
    "                priority_dict[matches].append(result)\n",
    "            else:\n",
    "                priority_dict[matches] = [result]\n",
    "    print(priority_dict)\n",
    "    priority_list = [val for key, val in sorted(priority_dict.items(), key=lambda x: x[0], reverse=True)]\n",
    "    #Sort by difflib relevancy. Possible that the following is enough? No need for match-based string subset ordering?\n",
    "    relevancy_list = [sorted(l, key=lambda seq: difflib.SequenceMatcher(None, seq, queries[0]).ratio(), reverse=True) for l in priority_list]\n",
    "    return list(chain(*relevancy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['TeamLab (art collective)']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TeamLab (art collective)']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = ['teamLab Borderless', 'tokyo']\n",
    "results = ['Team At Borderers', 'TeamLab (art collective)', 'Palette Town', 'James May: Our Man in Japan', 'Tactile technology', 'Themed Entertainment Association', 'List of Unity games', 'LG Electronics', 'Vietnam', 'Bayer', 'COVID-19 pandemic in Europe']\n",
    "\n",
    "relevancy_filter(queries, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, suggestion = wikipedia.search('kasairinkai park', suggestion=True)\n",
    "suggestion = ' '.join([word.capitalize() for word in suggestion.split()])\n",
    "results = [suggestion] + results\n",
    "print(results)\n",
    "#results = wikipedia.search(results[0])\n",
    "#print(results)\n",
    "wikipedia.WikipediaPage(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
