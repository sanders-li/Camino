{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3T4d77AJaKte"
   },
   "source": [
    "Windows adaptaion of semantic search with ANN and text embeddings \n",
    "\n",
    "Source:\n",
    "    https://www.tensorflow.org/hub/tutorials/tf2_semantic_approximate_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0jr0QK9qO5P"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-vBZiCCqld0"
   },
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NTYbdWcseuK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms import util\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import annoy\n",
    "from sklearn.random_projection import gaussian_random_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6Imq876rLWx"
   },
   "source": [
    "## 1. Download Sample Data\n",
    "\n",
    "[A Million News Headlines](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL#) dataset contains news headlines published over a period of 15 years sourced from the reputable Australian Broadcasting Corp. (ABC). This news dataset has a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on Australia. \n",
    "\n",
    "**Format**: Tab-separated two-column data: 1) publication date and 2) headline text. We are only interested in the headline text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INPWa4upv_yJ"
   },
   "outputs": [],
   "source": [
    "with open('corpus/text.txt', 'w') as out_file:\n",
    "  with open('raw.tsv', 'r') as in_file:\n",
    "    for line in in_file:\n",
    "      headline = line.split('\\t')[1].strip().strip('\"')\n",
    "      out_file.write(headline+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AngMtH50jNb"
   },
   "source": [
    "## 2. Generate Embeddings for the Data.\n",
    "\n",
    "In this tutorial, we use the [Neural Network Language Model (NNLM)](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) to generate embeddings for the headline data. The sentence embeddings can then be easily used to compute sentence level meaning similarity. We run the embedding generation process using Apache Beam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_DvXnDB1pEX"
   },
   "source": [
    "### Embedding extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yL7OEY1E0A35"
   },
   "outputs": [],
   "source": [
    "embed_fn = None\n",
    "\n",
    "def generate_embeddings(text, module_url, random_projection_matrix=None):\n",
    "  # Beam will run this function in different processes that need to\n",
    "  # import hub and load embed_fn (if not previously loaded)\n",
    "  global embed_fn\n",
    "  if embed_fn is None:\n",
    "    embed_fn = hub.load(module_url)\n",
    "  embedding = embed_fn(text).numpy()\n",
    "  if random_projection_matrix is not None:\n",
    "    embedding = embedding.dot(random_projection_matrix)\n",
    "  return text, embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6pXBVxsVUbm"
   },
   "source": [
    "### Convert to tf.Example method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMjqjWZNVVzd"
   },
   "outputs": [],
   "source": [
    "def to_tf_example(entries):\n",
    "  examples = []\n",
    "\n",
    "  text_list, embedding_list = entries\n",
    "  for i in range(len(text_list)):\n",
    "    text = text_list[i]\n",
    "    embedding = embedding_list[i]\n",
    "\n",
    "    features = {\n",
    "        'text': tf.train.Feature(\n",
    "            bytes_list=tf.train.BytesList(value=[text.encode('utf-8')])),\n",
    "        'embedding': tf.train.Feature(\n",
    "            float_list=tf.train.FloatList(value=embedding.tolist()))\n",
    "    }\n",
    "  \n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature=features)).SerializeToString(deterministic=True)\n",
    "  \n",
    "    examples.append(example)\n",
    "  \n",
    "  return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDiV4uQCVYGH"
   },
   "source": [
    "### Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCGUIB172m2G"
   },
   "outputs": [],
   "source": [
    "def run_hub2emb(args):\n",
    "  '''Runs the embedding generation pipeline'''\n",
    "\n",
    "  options = beam.options.pipeline_options.PipelineOptions(**args)\n",
    "  args = namedtuple(\"options\", args.keys())(*args.values())\n",
    "\n",
    "  with beam.Pipeline(args.runner, options=options) as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | 'Read sentences from files' >> beam.io.ReadFromText(\n",
    "            file_pattern=args.data_dir)\n",
    "        | 'Batch elements' >> util.BatchElements(\n",
    "            min_batch_size=args.batch_size, max_batch_size=args.batch_size)\n",
    "        | 'Generate embeddings' >> beam.Map(\n",
    "            generate_embeddings, args.module_url, args.random_projection_matrix)\n",
    "        | 'Encode to tf example' >> beam.FlatMap(to_tf_example)\n",
    "        | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n",
    "            file_path_prefix='{}/emb'.format(args.output_dir),\n",
    "            file_name_suffix='.tfrecords')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlbQdiYNVvne"
   },
   "source": [
    "### Generaring Random Projection Weight Matrix\n",
    "\n",
    "[Random projection](https://en.wikipedia.org/wiki/Random_projection) is a simple, yet powerfull technique used to reduce the dimensionality of a set of points which lie in Euclidean space. For a theoretical background, see the [Johnson-Lindenstrauss lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma).\n",
    "\n",
    "Reducing the dimensionality of the embeddings with random projection means less time needed to build and query the ANN index.\n",
    "\n",
    "In this tutorial we use [Gaussian Random Projection](https://en.wikipedia.org/wiki/Random_projection#Gaussian_random_projection) from the [Scikit-learn](https://scikit-learn.org/stable/modules/random_projection.html#gaussian-random-projection) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yw1xgtNVv52"
   },
   "outputs": [],
   "source": [
    "def generate_random_projection_weights(original_dim, projected_dim):\n",
    "  random_projection_matrix = None\n",
    "  random_projection_matrix = gaussian_random_matrix(\n",
    "      n_components=projected_dim, n_features=original_dim).T\n",
    "  print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n",
    "  print('Storing random projection matrix to disk...')\n",
    "  with open('random_projection_matrix', 'wb') as handle:\n",
    "    pickle.dump(random_projection_matrix, \n",
    "                handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "  return random_projection_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJZUfT3NE7kj"
   },
   "source": [
    "### Set parameters\n",
    "If you want to build an index using the original embedding space without random projection, set the `projected_dim` parameter to `None`. Note that this will slow down the indexing step for high-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "77-Cow7uE74T"
   },
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1' #@param {type:\"string\"}\n",
    "projected_dim = 64  #@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "On-MbzD922kb"
   },
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3I1Wv4i21yY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Gaussian random weight matrix was creates with shape of (128, 64)\n",
      "Storing random projection matrix to disk...\n",
      "Pipeline args are set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SandersLi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:86: FutureWarning: Function gaussian_random_matrix is deprecated; gaussian_random_matrix is deprecated in 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'job_name': 'hub2emb-200910-031802',\n",
       " 'runner': 'DirectRunner',\n",
       " 'batch_size': 1024,\n",
       " 'data_dir': 'corpus\\\\*.txt',\n",
       " 'output_dir': 'C:\\\\Users\\\\SANDER~1\\\\AppData\\\\Local\\\\Temp\\\\tmp2b7g77zp',\n",
       " 'module_url': 'https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1',\n",
       " 'random_projection_matrix': array([[ 0.14556211,  0.16797908,  0.19327695, ..., -0.10546049,\n",
       "          0.04348957, -0.20288548],\n",
       "        [ 0.08556691,  0.04585976, -0.06682875, ...,  0.08884578,\n",
       "          0.07781133,  0.16367863],\n",
       "        [-0.26843042, -0.20389443,  0.09223419, ...,  0.11595095,\n",
       "         -0.06493047, -0.00851031],\n",
       "        ...,\n",
       "        [-0.03193794, -0.20488071, -0.11304449, ...,  0.45732803,\n",
       "         -0.0279283 , -0.00914209],\n",
       "        [ 0.07162892,  0.00403485, -0.07360446, ...,  0.04260638,\n",
       "         -0.11348477,  0.11642548],\n",
       "        [ 0.15406356,  0.18406368, -0.09435445, ..., -0.21130506,\n",
       "         -0.02833571, -0.03901133]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "output_dir = tempfile.mkdtemp()\n",
    "original_dim = hub.load(module_url)(['']).shape[1]\n",
    "random_projection_matrix = None\n",
    "\n",
    "if projected_dim:\n",
    "  random_projection_matrix = generate_random_projection_weights(\n",
    "      original_dim, projected_dim)\n",
    "\n",
    "args = {\n",
    "    'job_name': 'hub2emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n",
    "    'runner': 'DirectRunner',\n",
    "    'batch_size': 1024,\n",
    "    'data_dir': 'corpus\\*.txt',\n",
    "    'output_dir': output_dir,\n",
    "    'module_url': module_url,\n",
    "    'random_projection_matrix': random_projection_matrix,\n",
    "}\n",
    "\n",
    "print(\"Pipeline args are set.\")\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iS9obmeP4ZOA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021337900D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021337900D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x00000213379001F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x00000213379001F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021337967670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021337967670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 20s\n",
      "Pipeline is done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running pipeline...\")\n",
    "%time run_hub2emb(args)\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JAwOo7gQWvVd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Blade\n",
      " Volume Serial Number is BEC2-DD53\n",
      "\n",
      " Directory of C:\\Users\\SANDER~1\\AppData\\Local\\Temp\\tmp2b7g77zp\n",
      "\n",
      "09/09/2020  08:35 PM    <DIR>          .\n",
      "09/09/2020  08:35 PM    <DIR>          ..\n",
      "09/09/2020  08:35 PM       388,699,429 emb-00000-of-00001.tfrecords\n",
      "               1 File(s)    388,699,429 bytes\n",
      "               2 Dir(s)  16,754,184,192 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "%ls {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVnee4e6U90u"
   },
   "source": [
    "Read some of the generated embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-K7pGXlXOj1N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline_text: [-0.03489685 -0.09948387 -0.05628211 -0.02369605 -0.10834836  0.10760727\n",
      " -0.18177474  0.03484242 -0.20400605 -0.12302925]\n",
      "aba decides against community broadcasting licence: [-0.03458044  0.13679439 -0.1517344   0.08366418  0.00353543 -0.01145421\n",
      "  0.22175474  0.06245604 -0.0542197  -0.25999856]\n",
      "act fire witnesses must be aware of defamation: [ 0.15315107  0.3754461  -0.18539304  0.2975436   0.01786217 -0.07028925\n",
      "  0.21732916  0.2267158  -0.09780349 -0.16126879]\n",
      "a g calls for infrastructure protection summit: [ 0.21369052  0.18222912 -0.07011453 -0.10433221  0.20290565  0.24954703\n",
      "  0.05606724 -0.06699353 -0.1903006  -0.00890797]\n",
      "air nz staff in aust strike for pay rise: [ 0.18976349  0.04245168 -0.1473818  -0.23294246  0.10474072  0.23743637\n",
      "  0.29843155 -0.0130834  -0.36882558  0.08112953]\n"
     ]
    }
   ],
   "source": [
    "embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords')\n",
    "sample = 5\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'text': tf.io.FixedLenFeature([], tf.string),\n",
    "    'embedding': tf.io.FixedLenFeature([projected_dim], tf.float32)\n",
    "}\n",
    "\n",
    "def _parse_example(example):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(embed_file)\n",
    "for record in dataset.take(sample).map(_parse_example):\n",
    "  print(\"{}: {}\".format(record['text'].numpy().decode('utf-8'), record['embedding'].numpy()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agGoaMSgY8wN"
   },
   "source": [
    "## 3. Build the ANN Index for the Embeddings\n",
    "\n",
    "[ANNOY](https://github.com/spotify/annoy) (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory. It is built and used by [Spotify](https://www.spotify.com) for music recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcPDspU3WjgH"
   },
   "outputs": [],
   "source": [
    "def build_index(embedding_files_pattern, index_filename, vector_length, \n",
    "    metric='angular', num_trees=100):\n",
    "  '''Builds an ANNOY index'''\n",
    "\n",
    "  annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)\n",
    "  # Mapping between the item and its identifier in the index\n",
    "  mapping = {}\n",
    "\n",
    "  embed_files = tf.io.gfile.glob(embedding_files_pattern)\n",
    "  num_files = len(embed_files)\n",
    "  print('Found {} embedding file(s).'.format(num_files))\n",
    "\n",
    "  item_counter = 0\n",
    "  for i, embed_file in enumerate(embed_files):\n",
    "    print('Loading embeddings in file {} of {}...'.format(i+1, num_files))\n",
    "    dataset = tf.data.TFRecordDataset(embed_file)\n",
    "    for record in dataset.map(_parse_example):\n",
    "      text = record['text'].numpy().decode(\"utf-8\")\n",
    "      embedding = record['embedding'].numpy()\n",
    "      mapping[item_counter] = text\n",
    "      annoy_index.add_item(item_counter, embedding)\n",
    "      item_counter += 1\n",
    "      if item_counter % 100000 == 0:\n",
    "        print('{} items loaded to the index'.format(item_counter))\n",
    "\n",
    "  print('A total of {} items added to the index'.format(item_counter))\n",
    "\n",
    "  print('Building the index with {} trees...'.format(num_trees))\n",
    "  annoy_index.build(n_trees=num_trees)\n",
    "  print('Index is successfully built.')\n",
    "  \n",
    "  print('Saving index to disk...')\n",
    "  annoy_index.save(index_filename)\n",
    "  print('Index is saved to disk.')\n",
    "  print(\"Index file size: {} GB\".format(\n",
    "    round(os.path.getsize(index_filename) / float(1024 ** 3), 2)))\n",
    "  annoy_index.unload()\n",
    "\n",
    "  print('Saving mapping to disk...')\n",
    "  with open(index_filename + '.mapping', 'wb') as handle:\n",
    "    pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  print('Mapping is saved to disk.')\n",
    "  print(\"Mapping file size: {} MB\".format(\n",
    "    round(os.path.getsize(index_filename + '.mapping') / float(1024 ** 2), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgyOQhUq6FNE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 embedding file(s).\n",
      "Loading embeddings in file 1 of 1...\n",
      "100000 items loaded to the index\n",
      "200000 items loaded to the index\n",
      "300000 items loaded to the index\n",
      "400000 items loaded to the index\n",
      "500000 items loaded to the index\n",
      "600000 items loaded to the index\n",
      "700000 items loaded to the index\n",
      "800000 items loaded to the index\n",
      "900000 items loaded to the index\n",
      "1000000 items loaded to the index\n",
      "1100000 items loaded to the index\n",
      "A total of 1103664 items added to the index\n",
      "Building the index with 100 trees...\n",
      "Index is successfully built.\n",
      "Saving index to disk...\n",
      "Index is saved to disk.\n",
      "Index file size: 1.6 GB\n",
      "Saving mapping to disk...\n",
      "Mapping is saved to disk.\n",
      "Mapping file size: 50.61 MB\n",
      "Wall time: 14min 17s\n"
     ]
    }
   ],
   "source": [
    "embedding_files = \"{}/emb-*.tfrecords\".format(output_dir)\n",
    "embedding_dimension = projected_dim\n",
    "index_filename = \"index\"\n",
    "\n",
    "!rm {index_filename}\n",
    "!rm {index_filename}.mapping\n",
    "\n",
    "%time build_index(embedding_files, index_filename, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ic31Tm5cgAd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Blade"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Volume Serial Number is BEC2-DD53\n",
      "\n",
      " Directory of C:\\Users\\SandersLi\\Camino\n",
      "\n",
      "09/09/2020  08:49 PM    <DIR>          .\n",
      "09/09/2020  08:49 PM    <DIR>          ..\n",
      "09/08/2020  02:32 AM               192 .gitignore\n",
      "09/09/2020  08:12 PM    <DIR>          .ipynb_checkpoints\n",
      "07/21/2020  08:12 PM    <DIR>          .vscode\n",
      "09/09/2020  08:15 PM                 0 asdf\n",
      "09/08/2020  06:51 PM    <DIR>          backend\n",
      "09/08/2020  10:29 PM    <DIR>          corpus\n",
      "07/27/2020  09:54 PM    <DIR>          env\n",
      "09/02/2020  03:04 PM    <DIR>          frontend\n",
      "09/09/2020  08:49 PM     1,722,824,064 index\n",
      "09/09/2020  08:49 PM        53,063,748 index.mapping\n",
      "07/27/2020  01:16 PM                24 Procfile\n",
      "09/09/2020  08:18 PM            65,673 random_projection_matrix\n",
      "09/08/2020  10:18 PM        57,600,231 raw.tsv\n",
      "07/24/2020  04:16 PM    <DIR>          static\n",
      "09/02/2020  03:04 PM    <DIR>          templates\n",
      "09/07/2020  02:53 AM           537,205 test.jpg\n",
      "09/09/2020  08:40 PM            33,999 tf2_semantic_approximate_nearest_neighbors.ipynb\n",
      "09/09/2020  08:38 PM             1,060 Untitled.ipynb\n",
      "              10 File(s)  1,834,126,196 bytes\n",
      "              10 Dir(s)  15,636,262,912 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maGxDl8ufP-p"
   },
   "source": [
    "## 4. Use the Index for Similarity Matching\n",
    "Now we can use the ANN index to find news headlines that are semantically close to an input query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dIs8W78fYPp"
   },
   "source": [
    "### Load the index and the mapping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlTTrbQHayvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy index is loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-4bcb1204a779>:1: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  index = annoy.AnnoyIndex(embedding_dimension)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping file is loaded.\n"
     ]
    }
   ],
   "source": [
    "index = annoy.AnnoyIndex(embedding_dimension)\n",
    "index.load(index_filename, prefault=True)\n",
    "print('Annoy index is loaded.')\n",
    "with open(index_filename + '.mapping', 'rb') as handle:\n",
    "  mapping = pickle.load(handle)\n",
    "print('Mapping file is loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6liFMSUh08J"
   },
   "source": [
    "### Similarity matching method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUxjTag8hc16"
   },
   "outputs": [],
   "source": [
    "def find_similar_items(embedding, num_matches=5):\n",
    "  '''Finds similar items to a given embedding in the ANN index'''\n",
    "  ids = index.get_nns_by_vector(\n",
    "  embedding, num_matches, search_k=-1, include_distances=False)\n",
    "  items = [mapping[i] for i in ids]\n",
    "  return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjerNpmZja0A"
   },
   "source": [
    "### Extract embedding from a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0IIXzfBjZ19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the TF-Hub module...\n",
      "Wall time: 2.95 s\n",
      "TF-Hub module is loaded.\n",
      "Loading random projection matrix...\n",
      "random projection matrix is loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the TF-Hub module\n",
    "print(\"Loading the TF-Hub module...\")\n",
    "%time embed_fn = hub.load(module_url)\n",
    "print(\"TF-Hub module is loaded.\")\n",
    "\n",
    "random_projection_matrix = None\n",
    "if os.path.exists('random_projection_matrix'):\n",
    "  print(\"Loading random projection matrix...\")\n",
    "  with open('random_projection_matrix', 'rb') as handle:\n",
    "    random_projection_matrix = pickle.load(handle)\n",
    "  print('random projection matrix is loaded.')\n",
    "\n",
    "def extract_embeddings(query):\n",
    "  '''Generates the embedding for the query'''\n",
    "  query_embedding =  embed_fn([query])[0].numpy()\n",
    "  if random_projection_matrix is not None:\n",
    "    query_embedding = query_embedding.dot(random_projection_matrix)\n",
    "  return query_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kCoCNROujEIO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1082 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021337900940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1082 calls to <function recreate_function.<locals>.restored_function_body at 0x0000021337900940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.02737208, -0.09724229, -0.03260606,  0.0102555 , -0.20648217,\n",
       "        0.15898153,  0.06123003,  0.0845266 ,  0.05975626,  0.04614805])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_embeddings(\"Hello Machine Learning!\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koINo8Du--8C"
   },
   "source": [
    "### Enter a query to find the most similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "wC0uLjvfk5nB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for the query...\n",
      "Wall time: 238 ms\n",
      "\n",
      "Finding relevant items in the index...\n",
      "Wall time: 4.2 ms\n",
      "\n",
      "Results:\n",
      "=========\n",
      "confronting global challenges\n",
      "the domestic challenges facing duterte\n",
      "chronology tsunamis of the past century\n",
      "how urgent is the rational scrutiny of religion\n",
      "lynch human rights movement needs better and bolder leaders\n",
      "emerging nations to help struggling global economy\n",
      "modern world encouraging rural and regional to seek help\n",
      "an small scale farming urged as solution to global hunger\n",
      "abbott says labor faces an existential crisis\n",
      "what are the biggest challenges facing new wa labor govt\n"
     ]
    }
   ],
   "source": [
    "#@title { run: \"auto\" }\n",
    "query = \"confronting global challenges\" #@param {type:\"string\"}\n",
    "\n",
    "print(\"Generating embedding for the query...\")\n",
    "%time query_embedding = extract_embeddings(query)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Finding relevant items in the index...\")\n",
    "%time items = find_similar_items(query_embedding, 10)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Results:\")\n",
    "print(\"=========\")\n",
    "for item in items:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkRSqs77tDuX"
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "You can learn more about TensorFlow at [tensorflow.org](https://www.tensorflow.org/) and see the TF-Hub API documentation at [tensorflow.org/hub](https://www.tensorflow.org/hub/). Find available TensorFlow Hub modules at [tfhub.dev](https://tfhub.dev/) including more text embedding modules and image feature vector modules.\n",
    "\n",
    "Also check out the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/) which is Google's fast-paced, practical introduction to machine learning."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "ACbjNjyO4f_8",
    "g6pXBVxsVUbm"
   ],
   "name": "Semantic Search with Approximate Nearest Neighbors and Text Embeddings from TF-Hub",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
